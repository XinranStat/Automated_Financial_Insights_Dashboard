# -*- coding: utf-8 -*-
"""fin_news_dashboard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qrFeTPckjshvY__sQTIuck1LmCQYrmi9
"""

# -*- coding: utf-8 -*-
"""fin_news_collector_v4.py - Integrates RSS scraping, AI processing, and PDF output"""

# ----------------------------------------------------
# Cell 1: Dependency Installation and RSS Scraping (Modified filtering logic)
# ----------------------------------------------------

# âš ï¸ Install required libraries: added reportlab for PDF generation
#!pip install google-genai feedparser pytz reportlab

# Import necessary libraries
import feedparser
import time
import pytz
from datetime import datetime, timedelta
import json
import logging
import os

# Import ReportLab PDF dependencies (to be used in Cell 3)
from reportlab.lib.pagesizes import A4
from reportlab.lib import colors
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.ttfonts import TTFont

# Configure logging
logging.basicConfig(level=logging.INFO)

# ==============================================================================
# ðŸ› ï¸ Configuration (RSS Feeds)
# ==============================================================================
RSS_CONFIG = [
    {
        "source": "The Economist (Finance & Economics)",
        "url": 'https://www.economist.com/finance-and-economics/rss.xml',
        "timezone": 'US/Pacific'
    },
    {
        "source": "The Economist (Business)",
        "url": 'https://www.economist.com/business/rss.xml',
        "timezone": 'US/Pacific'
    },
    {
        "source": "The Economist (united-states)",
        "url": 'https://www.economist.com/united-states/rss.xml',
        "timezone": 'US/Pacific'
    },
    {
        "source": "Financial Times (Markets)",
        "url": 'https://www.ft.com/markets?format=rss',
        "timezone": 'US/Pacific'
    }
]

CUSTOM_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

articles_data = []
DATA_FILENAME = "articles_data.json"

# ==============================================================================
# âš™ï¸ Core Functions (RSS Fetching & Filtering)
# ==============================================================================

def fetch_and_parse_rss(url):
    """Fetches and parses the specified RSS feed URL."""
    print(f"--- Attempting to parse RSS feed: {url} ---")
    feed = feedparser.parse(url, request_headers=CUSTOM_HEADERS)
    status = getattr(feed, 'status', 'N/A')
    if not feed.entries and status not in [200, 301, 302]:
        print(f"âš ï¸ Warning: Unable to fetch article list from {url}. Status code: {status}")
        return None
    if not feed.entries:
        print(f"âŒ Error: Feed object is empty, but status code is {status}. Connection might be refused or link is invalid.")
        return None
    return feed

def format_published_time(published_parsed, target_timezone_name=None):
    """Formats published time to 'YYYY-MM-DD HH:MM:SS (Timezone)' based on target timezone."""
    if not published_parsed:
        return 'No time info', None

    # Convert to UTC datetime object
    dt_utc = datetime(*published_parsed[:6], tzinfo=pytz.utc)

    if target_timezone_name:
        try:
            target_tz = pytz.timezone(target_timezone_name)
            dt_target = dt_utc.astimezone(target_tz)
            return dt_target.strftime('%Y-%m-%d %H:%M:%S %Z'), dt_utc
        except pytz.exceptions.UnknownTimeZoneError:
            pass # Fallback to UTC

    return dt_utc.strftime('%Y-%m-%d %H:%M:%S UTC'), dt_utc

def get_ft_cutoff():
    """Calculates cutoff time for Financial Times: 48 hours ago (UTC)."""
    return datetime.now(pytz.utc) - timedelta(hours=48)

def get_economist_cutoff():
    """
    Calculates cutoff time for The Economist: 90 days ago (UTC).
    """
    now = datetime.now(pytz.utc)
    # Subtract 90 days directly
    cutoff_date = now - timedelta(days=90)

    # Optional: align to 00:00:00 of the day
    # cutoff_date = cutoff_date.replace(hour=0, minute=0, second=0, microsecond=0)

    return cutoff_date

def should_keep_article(source_name, pub_dt_utc):
    """Determines if an article should be kept based on source and UTC time."""
    if pub_dt_utc is None:
        return False

    source = source_name.split(' (')[0].lower()

    # Financial Times: Within last 48 hours
    if 'financial times' in source:
        ft_cutoff = get_ft_cutoff()
        return pub_dt_utc > ft_cutoff

    # The Economist: Rolling 90 days
    elif 'economist' in source:
        eco_cutoff = get_economist_cutoff()
        return pub_dt_utc >= eco_cutoff

    # Keep by default
    return True


def process_feed_entries(feed, source_name, target_timezone_name=None, limit=None):
    """
    Iterates through feed entries, extracts info, applies time filtering, and adds to articles_data.
    """
    feed_title = feed.feed.get('title', 'N/A')
    print(f"\n## ðŸ“° Processing Feed: {feed_title} (Source: {source_name})")
    print(f"Total articles: {len(feed.entries)}.")

    extracted_count = 0
    filtered_out_count = 0

    for entry in feed.entries:
        title = entry.get('title', 'No Title')
        published_parsed = entry.get('published_parsed')

        # Format time and get UTC datetime for filtering
        formatted_time, pub_dt_utc = format_published_time(published_parsed, target_timezone_name)
        summary = entry.get('summary', 'No Summary').strip()
        link = entry.get('link', '#')

        # --- Core Filtering Logic ---
        if should_keep_article(source_name, pub_dt_utc):
            articles_data.append({
                "time": formatted_time,
                "published_parsed": published_parsed,
                "source": source_name.split(' (')[0],
                "title": title,
                "summary": summary,
                "link": link
            })
            extracted_count += 1
        else:
            filtered_out_count += 1

    print(f"âœ… Successfully extracted and kept {extracted_count} articles matching time criteria.")
    print(f"ðŸ—‘ï¸ Filtered out {filtered_out_count} expired articles.")
    print("-" * 40)

def save_to_json(data, filename=DATA_FILENAME):
    """Saves data to a local JSON file."""
    print(f"\nðŸ’¾ Saving data to {filename}...")
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        print("âœ… Data saved successfully.")
    except Exception as e:
        print(f"âŒ Storage failed: {e}")

# --- Main Execution: Execute Scraping ---
# Not using if __name__ == "__main__" for Colab/Jupyter compatibility
for config in RSS_CONFIG:
    source = config["source"]
    url = config["url"]
    target_tz = config["timezone"]

    feed_obj = fetch_and_parse_rss(url)

    if feed_obj:
        process_feed_entries(feed_obj, source, target_tz)

print(f"\nâœ¨ **All feeds processed. Collected {len(articles_data)} articles matching filters.**")
save_to_json(articles_data)

import os
import json
import logging
import sys
import time
import random
from dotenv import load_dotenv  # Added: Library to load .env files

# Try to import Google GenAI SDK
try:
    from google import genai
    from google.genai import types
except ImportError:
    print("Error: 'google-genai' library not found. Please run: pip install google-genai")
    sys.exit(1)

# --- API Key Initialization from .env ---
# Load variables from .env file into environment variables
load_dotenv()

# Attempt to get the key from environment variables
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

# Fallback for Colab users (keep original logic as backup)
if not GEMINI_API_KEY:
    try:
        from google.colab import userdata
        GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')
    except (ImportError, Exception):
        pass

# --- Initialize Client ---
client = None
if GEMINI_API_KEY:
    client = genai.Client(api_key=GEMINI_API_KEY)
    print("âœ… Gemini API Key successfully loaded from .env")
else:
    print("\nSevere Warning: GEMINI_API_KEY is not set. AI processing will be skipped.\n")

MODEL_NAME = "gemini-2.5-flash"

# =======================
# 2. PROMPT Definitions
# =======================

SYSTEM_PROMPT_STEP_1 = """
Role:
You are a Senior Financial Analyst and Editor for a top-tier US business publication. Your task is to curate and classify a raw feed of news articles based on their titles and one-sentence summaries.

Input Data:
A JSON list of articles. Each article has an 'id', 'title', and 'summary'.

Classification Categories (Select One Only):
1. Macro (Macroeconomics & Policy): US inflation, Federal Reserve (rates/Powell), US labor market, taxes, housing market trends, GDP, and broad economic sentiment.
2. Tech/AI (Technology & Artificial Intelligence): AI advancements, chip industry (Nvidia/semiconductors), tech regulation/antitrust, startups, impact of AI on jobs, and major tech product shifts.
3. Geopolitics (Global Affairs & Trade): US foreign policy, trade wars (tariffs), sanctions, major conflicts involving US interests (Ukraine, Israel, China-US relations), and elections with global impact.
4. Corporate Finance/VC (Markets & Deals): Stock market movements, M&A (mergers & acquisitions), IPOs, Venture Capital funding, Crypto/Blockchain (as an asset class), CEO changes, bankruptcy/restructuring, and private equity.

Strict Filtering Rules (The "Gatekeeper"):
- US Nexus Filter: KEEP only articles that are relevant to the US market, US companies, US policy, or global events that directly impact US interests.
- DISCARD purely local news from other regions (e.g., "European penmakers struggling", "Vietnam's local EV issues" unless they have a direct US listing/impact).

Conservative Logic (First-Order Implications Only):
- Do NOT over-extrapolate.
- Exclusivity: If an article fits two categories, choose the dominant angle.
- Tie-breaker: If it's a Tech company stock moving, put in Corp Finance if the focus is the stock price; put in Tech/AI if the focus is the product/technology.

Output Format:
Return a STRICT JSON object with four keys: "Macro", "Tech/AI", "Geopolitics", "Corporate Finance/VC".
The value for each key should be a list of the *integer IDs* of the articles that belong to that category.
Do NOT include the full article text in the output, ONLY the IDs.
If an article is filtered out (irrelevant), do not include its ID in any list.
""" # Ensure this is properly closed

SYSTEM_PROMPT_STEP_2 = """
Role:
You are a Senior Strategy Analyst for a top US Tech company. Your stakeholders are **Tech Executives (CTO, VP)** and **Industry Practitioners**.

Task:
Analyze the provided news articles and assign a **Continuous Sentiment Score** (from -1.0 to 1.0) based on **Operational & Strategic Impact**.

Scoring Methodology (The Operator's Lens):
- **Focus:** operational friction vs. velocity, talent density, supply chain stability, and regulatory environment.
- **Ignore:** Day-to-day stock price fluctuations (unless indicating systemic failure).

Scale Definitions:
- **[-1.0 to -0.6] Existential/Severe Threat:** Sanctions, supply chain cut-off, disruptive competitor breakthrough, severe regulatory bans.
- **[-0.5 to -0.1] Operational Friction:** Hiring difficulties, rising cloud costs, minor compliance headaches, non-core layoffs.
- **[0.0] Noise/Neutral:** Routine earnings calls (priced in), vague macro sentiments, political slogans with no policy details.
- **[+0.1 to +0.5] Operational Benefit:** Efficiency gains, competitor stumble, clear compliance guidelines, talent supply increase.
- **[+0.6 to +1.0] Strategic Opportunity:** Major tech breakthrough (enabler), open ecosystem releases, massive new demand unlock.

Input Data:
A JSON list of articles with 'id', 'title', 'summary', and 'section'.

Output Format:
Return a STRICT JSON object mapping the article 'id' (string) to its 'sentiment_score' (float).
Example: {"101": 0.5, "102": -0.8}
""" # Ensure this is properly closed

# =======================
# 3. Core Processing Function (with Retry Logic)
# =======================

def call_gemini_with_retry(system_prompt: str, user_content: str, max_retries=5):

    if not client:
        # Log this warning for better debugging
        logging.warning("AI Client not initialized, skipping Gemini API call.")
        return {}

    for attempt in range(max_retries):
        try:
            response = client.models.generate_content(
                model=MODEL_NAME,
                contents=user_content,
                config=types.GenerateContentConfig(
                    system_instruction=system_prompt,
                    response_mime_type='application/json',
                    temperature=0.0
                )
            )
            return json.loads(response.text)
        except Exception as e:
            err_msg = str(e)
            if "429" in err_msg or "RESOURCE_EXHAUSTED" in err_msg:
                # Exponential backoff: wait time increases with attempts
                wait_time = (2 ** attempt) + random.uniform(15, 25)
                print(f"Limit triggered (429/TPM), retrying ({attempt+1}/{max_retries}), waiting {wait_time:.1f}s...")
                time.sleep(wait_time)
            else:
                print(f"Error occurred: {e}")
                return {}
    logging.error("Max retries reached, API call failed.")
    return {}

# =======================
# 4. Main Execution Flow
# =======================

def main():
    input_file = 'articles_data.json'
    output_file = 'tagged_articles_analyzed.json'

    if not os.path.exists(input_file):
        print(f"File not found: {input_file}")
        return

    with open(input_file, 'r', encoding='utf-8') as f:
        raw_articles = json.load(f)

    for idx, a in enumerate(raw_articles):
        a['news_id'] = idx

    print(f"Loaded articles: {len(raw_articles)} total.")

    # --- STEP 1: Classification ---
    print("Executing Step 1: Classification and Filtering...")
    step1_input = [{"id": a['news_id'], "title": a['title'], "summary": a['summary']} for a in raw_articles]

    class_res = call_gemini_with_retry(SYSTEM_PROMPT_STEP_1, json.dumps(step1_input))
    if not class_res:
        print("Step 1 classification failed or returned empty results.")
        return # Exit if classification fails

    classified_map = {}
    kept_ids = set()
    # Check if class_res has expected structure (keys are categories, values are lists of IDs)
    for section, ids in class_res.items():
        if isinstance(ids, list): # Ensure 'ids' is iterable
            for nid in ids:
                classified_map[nid] = section
                kept_ids.add(nid)
        else:
            print(f"Warning: Step 1 returned unexpected format for section '{section}'. Skipping.")


    print(f"âœ… Step 1 complete. Kept: {len(kept_ids)} articles.")

    # Mandatory cooldown of 10s to let TPM counter subside
    if kept_ids:
        print("Cooling down for 10 seconds to protect TPM quota...")
        time.sleep(10)

    # --- STEP 2: Sentiment Scoring ---
    print("Executing Step 2: Strategic Sentiment Scoring...")
    step2_input = []
    for a in raw_articles:
        if a['news_id'] in kept_ids:
            step2_input.append({
                "id": a['news_id'], "title": a['title'],
                "summary": a['summary'], "section": classified_map[a['news_id']]
            })

    sent_res = {}
    if step2_input:
        sent_res = call_gemini_with_retry(SYSTEM_PROMPT_STEP_2, json.dumps(step2_input))
    else:
        print("No articles passed Step 1 filtering, skipping Step 2 scoring.")

    if not sent_res:
        print("Step 2 sentiment scoring failed or returned empty results.")
        # Proceed with an empty sent_res, meaning no articles will be added to final_output
        # or just return if no output is desired


    # --- Final Assembly ---
    final_output = []
    for a in raw_articles:
        nid = a['news_id']
        str_nid = str(nid) # API returns IDs as strings

        # Only add if it was classified AND sentiment scored
        if nid in kept_ids and str_nid in sent_res:
            final_output.append({
                "news_id": nid,
                "time": a.get("time", ""), # Keep time variable
                "source": a.get("source", "Unknown"),
                "title": a["title"],
                "summary": a["summary"],
                "section": classified_map[nid],
                "sentiment_score": sent_res[str_nid],
                "link": a.get("link", "")
            })

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, indent=4, ensure_ascii=False)

    print(f"Processing complete! Results saved to: {output_file} ({len(final_output)} articles total)")

if __name__ == "__main__":
    main()

import pandas as pd
import json

# Define the filename for the processed articles
PROCESSED_ARTICLES_FILENAME = 'tagged_articles_analyzed.json'

# Load the JSON data into a pandas DataFrame
try:
    with open(PROCESSED_ARTICLES_FILENAME, 'r', encoding='utf-8') as f:
        articles_df = pd.DataFrame(json.load(f))
    print(f"Successfully loaded {len(articles_df)} articles from {PROCESSED_ARTICLES_FILENAME}.")
    # display(articles_df.head())  # Commented out: display() is Jupyter/Colab only
    # Create date variable based on time
    # Strip timezone abbreviations (PST, PDT, UTC, etc.) before parsing,
    # as newer pandas versions no longer support them directly
    articles_df['date_PST'] = pd.to_datetime(
        articles_df['time'].str.replace(r'\s+[A-Z]{2,4}$', '', regex=True)
    ).dt.date
    # save the file
    articles_df.to_csv('articles_tagged.csv', index=False)
except FileNotFoundError:
    print(f"Error: The file {PROCESSED_ARTICLES_FILENAME} was not found.")
    articles_df = pd.DataFrame() # Create an empty DataFrame to avoid errors later
except json.JSONDecodeError:
    print(f"Error: Could not decode JSON from {PROCESSED_ARTICLES_FILENAME}. Check file integrity.")
    articles_df = pd.DataFrame() # Create an empty DataFrame
except Exception as e:
    print(f"An unexpected error occurred: {e}")
    articles_df = pd.DataFrame() # Create an empty DataFrame

import yfinance as yf
import pandas as pd
import matplotlib.pyplot as plt

# 1. Download Data
tickers = ["^NDX", "^TNX"]
finance_metrics = yf.download(tickers, period="3mo", interval="1d")['Close']

# 2. Calculations
# Nasdaq 100 Daily % Return
finance_metrics['NDX_Ret'] = finance_metrics['^NDX'].pct_change() * 100

# Treasury 10Y Daily Basis Point Change
finance_metrics['TNX_Bps'] = (finance_metrics['^TNX'] - finance_metrics['^TNX'].shift(1)) * 100

# 3. The Signal Column
# We flag days where NDX moves > 1.5% OR TNX moves > 10 bps
finance_metrics['Significant_Move'] = (finance_metrics['NDX_Ret'].abs() > 1.5) | (finance_metrics['TNX_Bps'].abs() > 10)

# save the file
finance_metrics.to_csv('finance_metrics.csv')

import pandas as pd
import textwrap
from datetime import datetime

# 1. Load Data
try:
    # Attempting to load the analyzed JSON file from the previous step
    # (Changed to .json to match your previous step's output 'tagged_articles_analyzed.json')
    df = pd.read_json('tagged_articles_analyzed.json')
except (FileNotFoundError, ValueError):
    try:
        df = pd.read_csv('articles_tagged.csv')
    except FileNotFoundError:
        print("Error: Input data file not found.")
        df = pd.DataFrame()

if not df.empty:
    # ---------------------------------------------------------
    # 2. Generate _basic_info.md
    # ---------------------------------------------------------
    current_date = datetime.now().strftime('%Y-%m-%d')
    total_processed = len(df)
    refresh_frequency = "Daily"

    basic_info_content = f"""
| Date | Processed Articles | Refresh Frequency |
| :--- | :---: | :---: |
| {current_date} | **{total_processed}** | {refresh_frequency} |
"""

    with open("_basic_info.md", "w", encoding='utf-8') as f:
        f.write(basic_info_content)
    print("âœ… _basic_info.md updated")

    # ---------------------------------------------------------
    # 3. Generate _48h-Market Sentiment.md
    # ---------------------------------------------------------
    avg_sentiment = df['sentiment_score'].mean()
    # Check if 'section' column exists to determine the top driver section
    top_section = df['section'].mode()[0] if 'section' in df.columns else "N/A"

    market_sentiment_content = f"""
| Avg Market Sentiment Score | Top Driver Section |
| :---: | :---: |
| **{avg_sentiment:.2f}** | **{top_section}** |
"""

    with open("_48h-Market-Sentiment.md", "w", encoding='utf-8') as f:
        f.write(market_sentiment_content)
    print("âœ… _48h-Market-Sentiment.md updated")

    # ---------------------------------------------------------
    # 4. Generate _top_drivers.md
    # ---------------------------------------------------------
    # Sort by absolute value of sentiment score to find most impactful articles
    top_3 = df.sort_values(by='sentiment_score', key=abs, ascending=False).head(3)

    driver_rows = []
    for _, row in top_3.iterrows():
        # Safely extract fields
        summary = str(row.get('summary', 'No summary available'))
        title = row.get('title', 'No Title')
        # Support both 'url' or 'link' field names
        link = row.get('url', row.get('link', '#'))
        section = row.get('section', 'General')
        score = row.get('sentiment_score', 0.0)

        # Text wrapping for cleaner table display
        summary_text = textwrap.fill(summary, width=80).replace('\n', '<br>')

        # Format HTML link and sub-text
        details = (f"<b><a href='{link}' target='_blank' style='text-decoration:none; color:#0052cc;'>{title}</a></b><br>"
                   f"<span style='color: #666; font-size: 0.9em;'><i>{summary_text}</i></span>")

        driver_rows.append({
            "Article Details": details,
            "Section": f"`{section}`", # Use code block style for emphasis
            "Score": f"**{score:.2f}**" # Bold the score
        })

    # Save as Markdown table with prefix
    if driver_rows:
        pd.DataFrame(driver_rows).to_markdown("_top_drivers.md", index=False)
        print("âœ… _top_drivers.md updated")
    else:
        with open("_top_drivers.md", "w", encoding='utf-8') as f:
            f.write("No data available.")

else:
    print("DataFrame is empty. Skipping generation.")

import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import timedelta
import textwrap

# ==========================================
# 1. Data Loading & Preprocessing
# ==========================================

# Read financial metrics
finance_df = pd.read_csv('finance_metrics.csv')
finance_df['Date'] = pd.to_datetime(finance_df['Date']).dt.date

# Read news articles
articles_df = pd.read_csv('articles_tagged.csv')
articles_df['Date'] = pd.to_datetime(articles_df['date_PST']).dt.date

# Initialize sentiment scores if missing
if 'sentiment_score' not in articles_df.columns:
    articles_df['sentiment_score'] = 0
articles_df['abs_sentiment'] = articles_df['sentiment_score'].abs()

# ==========================================
# 2. Core Logic: [T-1, T+1] Window + Text Wrapping
# ==========================================

def get_context_news(row):
    # If not a significant move day, return None to keep hover space clean
    if not row['Significant_Move']:
        return None

    target_date = row['Date']
    start_date = target_date - timedelta(days=1)
    end_date = target_date + timedelta(days=1)

    # Filter articles within the 3-day window
    mask = (articles_df['Date'] >= start_date) & (articles_df['Date'] <= end_date)
    window_news = articles_df.loc[mask].copy()

    if window_news.empty:
        return "No relevant news found in [T-1, T+1] window."

    # Select top 3 articles by impact (absolute sentiment)
    top_news = window_news.sort_values(by='abs_sentiment', ascending=False).head(3)

    formatted_list = []
    for _, article in top_news.iterrows():
        date_str = article['Date'].strftime('%Y-%m-%d')
        raw_title = article['title']

        # Wrap text at 60 characters for readability in Hover UI
        # replace('\n', '<br>') ensures HTML rendering of line breaks
        wrapped_title = textwrap.fill(raw_title, width=60).replace('\n', '<br>            ')

        formatted_list.append(f"[{date_str}] {wrapped_title}")

    # Add a clear section header for trigger events
    header = "<br><b>Trigger Event Context:</b>"
    return header + "<br>" + '<br>'.join(formatted_list)

# Apply context function
finance_df['Hover_Text'] = finance_df.apply(get_context_news, axis=1)
# Fill NaN with empty strings to prevent Plotly rendering errors
finance_df['Hover_Text'] = finance_df['Hover_Text'].fillna("")

# ==========================================
# 3. Plotly Visualization (Optimized Styles)
# ==========================================

fig = make_subplots(specs=[[{"secondary_y": True}]])

# --- Trace 1: Nasdaq 100 (Primary Y-Axis) ---
# Color: "Institutional Blue"
fig.add_trace(
    go.Scatter(
        x=finance_df['Date'],
        y=finance_df['NDX_Ret'],
        name="Nasdaq 100 Return",
        mode='lines+markers',
        line=dict(color='#0052CC', width=2),
        marker=dict(size=4),
        # Display individual data in the unified hover
        hovertemplate="<b>Nasdaq 100 Ret</b>: %{y:.2f}%<extra></extra>"
    ),
    secondary_y=False,
)

# --- Trace 2: 10Y Treasury Yield (Secondary Y-Axis) ---
# Color: "Alert Orange/Red"
# News Context is bound to this trace to appear at the bottom of the unified hover
fig.add_trace(
    go.Scatter(
        x=finance_df['Date'],
        y=finance_df['^TNX'],
        name="10Y Treasury Yield",
        mode='lines',
        line=dict(dash='dot', color='#FF5733', width=2),
        customdata=finance_df['Hover_Text'], # Bind processed news text
        hovertemplate="<b>10-Year Yield</b>: %{y:.2f}%" +
                      "%{customdata}<extra></extra>"
    ),
    secondary_y=True,
)

# --- Trace 3: Significant Move Indicators (Red Circles) ---
sig_moves = finance_df[finance_df['Significant_Move'] == True]
fig.add_trace(
    go.Scatter(
        x=sig_moves['Date'],
        y=sig_moves['NDX_Ret'],
        mode='markers',
        marker=dict(
            color='#E74C3C', # High-contrast red
            size=12,
            symbol='circle-open', # Open circle to avoid obscuring data points
            line=dict(width=3)
        ),
        name="Significant Market Move",
        hoverinfo='skip' # Skip individual hover to avoid redundancy in unified mode
    ),
    secondary_y=False
)

# ==========================================
# 4. Layout & Aesthetics (Clean Minimalist Style)
# ==========================================

fig.update_layout(
    template="plotly_white",
    hovermode="x unified", # Combine all trace info at a single X-coordinate

    # Legend: Horizontal placement at the top
    legend=dict(
        orientation="h",
        y=1.05,
        x=0.0,
        xanchor="left",
        yanchor="bottom",
        bgcolor='rgba(255,255,255,0)'
    ),

    # Tight margins for report integration
    margin=dict(l=10, r=10, t=30, b=10),

    # Hover Label Styling
    hoverlabel=dict(
        bgcolor="rgba(255, 255, 255, 0.95)",
        font_size=13,
        font_family="Arial",
        align="left"
    )
)

# Axis Refinement
fig.update_xaxes(showgrid=True, gridcolor='rgba(0,0,0,0.05)')
fig.update_yaxes(title_text="Nasdaq 100 Return (%)", secondary_y=False, showgrid=True, gridcolor='rgba(0,0,0,0.05)')
fig.update_yaxes(title_text="10Y Treasury Yield (%)", secondary_y=True, showgrid=False)

# Export to HTML snippet
fig.write_html("market_plot.html", full_html=False, include_plotlyjs='cdn')
print("Plot generated successfully with corrected labels.")

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker

# ==========================================
# 1. Style Configuration
# ==========================================
sns.set_theme(style="white", context="talk", font_scale=0.9)
plt.rcParams['figure.figsize'] = (8, 8)

# ==========================================
# 2. Data Loading & Preprocessing
# ==========================================
df = pd.read_csv('articles_tagged.csv')

if not df.empty:
    df['time'] = pd.to_datetime(df['time'].astype(str).str.replace(r'\s+[A-Z]{2,4}$', '', regex=True))
    df['Date'] = df['time'].dt.date

# -----------------------------------------------------------
    # CHART 1: Volume by Section (Bar Chart)
    # -----------------------------------------------------------
    plt.figure(figsize=(8, 8))
    volume_counts = df['section'].value_counts().reset_index()
    volume_counts.columns = ['Section', 'Volume']


    ax = sns.barplot(
        data=volume_counts,
        x='Section',
        y='Volume',
        palette='Blues',
        alpha=0.9,
        linewidth=0
    )

    plt.title("Article Volume by Section", pad=25, fontweight='bold', fontsize=16, color='#333333')
    plt.xlabel("")
    plt.ylabel("Count of Articles", color='gray')


    sns.despine(left=True, bottom=True, right=True, top=True)

    ax.tick_params(axis='both', which='both', length=0)
    plt.xticks(color='gray')
    plt.yticks(color='gray')

    plt.grid(axis='y', linestyle='--', alpha=0.2)

    plt.tight_layout()
    plt.savefig("volume_by_section.png", dpi=200)

    # -----------------------------------------------------------
    # CHART 2: Sentiment Heatmap (Optimized Space & Dates)
    # -----------------------------------------------------------
    plt.figure(figsize=(8, 8))

    # Prepare Pivot Table
    daily_sentiment = df.groupby(['Date', 'section'])['sentiment_score'].mean().reset_index()
    heatmap_data = daily_sentiment.pivot_table(
        index='section',
        columns='Date',
        values='sentiment_score'
    )

    # Ensure consistent category order
    all_sections = ["Corporate Finance/VC", "Macro", "Tech/AI", "Geopolitics"]
    heatmap_data = heatmap_data.reindex(all_sections)

    # Draw Heatmap
    # shrink=0.4: Makes the colorbar much smaller
    # fraction=0.046: Standard ratio to prevent plot squashing
    ax = sns.heatmap(
        heatmap_data,
        cmap='RdYlGn',
        vmin=-1,
        vmax=1,
        annot=False,
        cbar_kws={
            'label': 'Sentiment Score',
            'shrink': 0.4,
            'fraction': 0.046,
            'pad': 0.04
        },
        linewidths=0.5,
        linecolor='white'
    )

    plt.title("Daily Sentiment Heatmap", pad=25, fontweight='bold', fontsize=16)
    plt.xlabel("")
    plt.ylabel("")

    # --- Fixed Date Formatting: Prevent Overlap ---
    num_dates = len(heatmap_data.columns)

    # Use MaxNLocator to force show only a maximum of 8 dates horizontally
    # This automatically scales whether you have 14 days or 90 days
    ax.xaxis.set_major_locator(ticker.MaxNLocator(nbins=8))

    # Get the dates based on the new locator positions
    # We map the fixed positions back to our date columns
    xticks = ax.get_xticks()
    date_labels = []
    for x in xticks:
        idx = int(x)
        if 0 <= idx < num_dates:
            date_labels.append(heatmap_data.columns[idx].strftime('%m-%d'))
        else:
            date_labels.append("")

    ax.set_xticklabels(date_labels, rotation=0, ha='center')

    plt.tight_layout()
    plt.savefig("sentiment_heatmap.png", dpi=200)
    print("âœ… Success: Heatmap updated with smaller colorbar and non-overlapping dates.")

else:
    print("âŒ No data available for processing.")

import pandas as pd
from google import genai
from google.genai import types
import sys

# 1. Prepare Data
try:
    df = pd.read_csv('articles_tagged.csv')

    # Filter for 'The Economist'
    economist_df = df[df['source'].str.contains('The Economist', case=False, na=False)].copy()

except Exception as e:
    print(f"Data Load Error: {e}")
    economist_df = pd.DataFrame()

# Check if data exists
if economist_df.empty:
    print("No Economist data found.")
    with open("_strategy_analysis.md", "w", encoding="utf-8") as f:
        f.write("No data available.")
else:
    # Simplify input for the API
    input_data = ""

    # Group by Section
    for section, group in economist_df.groupby('section'):
        # --- CRITICAL CHANGE: Removed [:3] limit ---
        # Now takes ALL titles in the group
        titles = " | ".join(group['title'].tolist())
        input_data += f"\nSection: {section}\nTitles: {titles}\n"

    print(f"Data prepared: Analyzing {len(economist_df)} articles across {len(economist_df['section'].unique())} sections.")

    # 2. Single API Call
    if 'client' in globals() and client:
        try:
            print("Generating analysis with YOUR prompt...")

            # Ensure MODEL_NAME is defined (e.g., "gemini-2.5-flash")
            # If not defined, fallback or ensure you defined it in previous cell
            target_model = globals().get('MODEL_NAME', 'gemini-2.5-flash')

            response = client.models.generate_content(
                model=target_model,
                config=types.GenerateContentConfig(
                    system_instruction=(
                        "You are a Senior Strategy Analyst for US Tech Industry. "
                        "Your stakeholders are Tech Executives (CTO, VP) and Industry Practitioners. "
                        "Your tone is clinical, professional, and highly conservative. "
                        "Perform ONLY first-order inferences based on the facts provided."
                        "Tone: Clinical and concise. "
                        "Constraint: Exactly 1 bullet point per category. Max 20 words per bullet."
                    ),
                    temperature=0.1
                ),
                contents=(
                    f"Analyze these Economist articles. For each Section, provide 1-bullet Opportunities, "
                    f"1-bullet Risks, and 1-bullet Watchlist. Use first-order inferences only. "
                    f"Format as Markdown (## for Section, ### for headers).\n\n"
                    f"DATA:\n{input_data}"
                )
            )

            # 3. Save for Quarto include
            if response.text:
                # Add spacing for better Markdown rendering
                final_markdown = response.text.replace("###", "\n\n###")

                with open("_strategy_analysis.md", "w", encoding="utf-8") as f:
                    f.write(final_markdown)
                print("âœ… Concise analysis generated (Format fixed for Quarto).")
            else:
                print("âŒ Error: Empty response from API.")

        except Exception as e:
            print(f"API Error: {e}")
    else:
        print("Client not initialized. Please ensure google.genai.Client is set up.")

import pandas as pd
from google import genai
from google.genai import types
import sys
import os
import time

# --- CONFIGURATION ---
MODEL_NAME = "gemini-2.5-flash"
OUTPUT_FILE = "_Tech-Strategy-Matrix.md"

# --- 0. SAFETY COOL-DOWN ---
print("â³ Clearing API rate limits (Waiting 10s)...")
time.sleep(10)
print("âœ… Ready.")

# --- 1. PREPARE DATA ---
try:
    df = pd.read_csv('articles_tagged.csv')

    # Clean data
    df = df.dropna(subset=['title'])
    df = df.drop_duplicates(subset=['title'])

    # Take ALL titles
    all_titles = df['title'].tolist()
    full_text_data = "\n".join(all_titles)

    print(f"ðŸ“Š Analyzing {len(all_titles)} headlines.")

except Exception as e:
    print(f"âŒ Data Load Error: {e}")
    full_text_data = ""

# --- 2. DEFINE PROMPT (Refined Constraints) ---
if not full_text_data:
    print("No data to process.")
else:
    system_instruction = """
    You are a Senior Tech Strategy Analyst.
    Your goal: Synthesize a "Tech-Strategy Matrix" from the provided news headlines.

    Target Audience: Engineering Practitioners & Business Executives.
    Tone: Clinical, concise, no fluff.

    CONSTRAINT: Perform ONLY first-order inferences based on the facts provided. Do NOT speculate beyond the immediate data.

    MANDATORY OUTPUT FORMAT:
    - Single Markdown Table.
    - NO Emojis.
    - Columns: Keyword | Trend | Tech Reality | Biz Impact

    COLUMN DEFINITIONS:
    1. Keyword: The specific topic (Big 5 only).
    2. Trend: The directional shift. STRICT LIMIT: Less than 6 words.
    3. Tech Reality: The engineering truth (Max 20 words).
    4. Biz Impact: The business consequence (Max 20 words).

    MANDATORY KEYWORDS (The Big 5):
    1. Agents
    2. Inference
    3. GPU
    4. ROI
    5. Regulation

    If headlines are missing for a keyword, infer the general industry consensus using strict first-order logic.
    """

    user_prompt = f"""
    Analyze these {len(all_titles)} headlines and generate the matrix.

    HEADLINES:
    {full_text_data}
    """

    # --- 3. SINGLE API CALL ---
    if 'client' in globals() and client:
        try:
            print("ðŸš€ Sending request to Gemini...")

            response = client.models.generate_content(
                model=MODEL_NAME,
                config=types.GenerateContentConfig(
                    system_instruction=system_instruction,
                    temperature=0.1,
                ),
                contents=user_prompt
            )

            if response.text:
                with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
                    f.write(response.text)
                print(f"âœ… Success! Matrix saved to: {OUTPUT_FILE}")
                print(f"   Content preview:\n{response.text[:200]}...")
            else:
                print("âŒ Error: Empty response.")

        except Exception as e:
            print(f"âŒ API Error: {e}")
            if "429" in str(e):
                print("âš ï¸ Still hitting rate limits. Wait 2-3 minutes.")
    else:
        print("âŒ Client not initialized.")